{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zFNCrD6xrm5z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import vgg16\n",
        "import time\n",
        "import os.path as osp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSVIiB0ysMZQ",
        "outputId": "49619611-bace-47a2-ebf2-e7b71293ee1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZYgtAFmrpU6",
        "outputId": "3399bc46-97d5-454c-eaa1-63acd674481e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data_cifar100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 69875905.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data_cifar100/cifar-100-python.tar.gz to ./data_cifar100\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Loading dataset and pre-processing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "trainset = CIFAR100(root='./data_cifar100', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = CIFAR100(root='./data_cifar100', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wu2yg4atNth",
        "outputId": "8c350664-007a-44dc-bd2d-c5258c83cc9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# net = vgg16(pretrained=False, num_classes=100)  # CIFAR-100 has 100 classes\n",
        "# net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg5ISSTcrt51"
      },
      "outputs": [],
      "source": [
        "# Building VGG16\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 10)  # CIFAR-10 has 10 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = VGG16()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lTY0k32xrycD"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_-yQthgusJff"
      },
      "outputs": [],
      "source": [
        "log_file_path = \"/content/gdrive/MyDrive/Project_II/model_logging/VGG_16_cifar100.log\"\n",
        "\n",
        "# definition a function to write data to log file\n",
        "def write_log(log_file_path, content):\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "        log_file.write(content + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FziZLwIJ_oFa"
      },
      "outputs": [],
      "source": [
        "# definition a function to read data from log file\n",
        "def read_log(log_file_path):\n",
        "    with open(log_file_path, 'r') as log_file:\n",
        "        lines = log_file.readlines()\n",
        "\n",
        "    # Check if log file contains data or being empty\n",
        "    if len(lines) == 0:\n",
        "        return 0, None\n",
        "\n",
        "    # Get the number of epochs has been completed\n",
        "    num_epochs_completed = len(lines) // 3\n",
        "\n",
        "    # Check if the remained lines of the log file\n",
        "    if len(lines) < 3 or len(lines) % 3 != 0:\n",
        "        print(\"Error: File log contains incomplete or invalid information.\")\n",
        "        return num_epochs_completed, None\n",
        "\n",
        "    try:\n",
        "        # Get the information of last epoch\n",
        "        last_epoch_info = lines[-3:]\n",
        "        last_epoch_train_loss = float(last_epoch_info[0].split(\":\")[1].strip())\n",
        "        last_epoch_train_accuracy = float(last_epoch_info[1].split(\":\")[1].strip()[:-1])\n",
        "    except (ValueError, IndexError) as e:\n",
        "        print(\"Error: Invalid format or missing information in log file.\")\n",
        "        return num_epochs_completed, None\n",
        "\n",
        "    return num_epochs_completed, (last_epoch_train_loss, last_epoch_train_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X5KLs2SsAbU3"
      },
      "outputs": [],
      "source": [
        "# Intialize the array to save training time per each epoch\n",
        "epoch_times_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TaZH6RoXAb5P"
      },
      "outputs": [],
      "source": [
        "# Starting to calculate training model time\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JKDsGuu5AHP3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLHS642GsTHu",
        "outputId": "753ab667-5f67-4d46-f946-ddf080a56291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 4.514012345267684, Train Accuracy: 1.63%\n",
            "Accuracy on test set: 2.30%\n",
            "Epoch 2, Loss: 4.3304688104278295, Train Accuracy: 3.27%\n",
            "Accuracy on test set: 4.47%\n",
            "Epoch 3, Loss: 3.9457928768509185, Train Accuracy: 7.06%\n",
            "Accuracy on test set: 10.69%\n",
            "Epoch 4, Loss: 3.6383996845206337, Train Accuracy: 11.76%\n",
            "Accuracy on test set: 15.08%\n",
            "Epoch 5, Loss: 3.3309383712461234, Train Accuracy: 17.33%\n",
            "Accuracy on test set: 21.94%\n",
            "Epoch 6, Loss: 3.012654499019808, Train Accuracy: 23.45%\n",
            "Accuracy on test set: 24.84%\n",
            "Epoch 7, Loss: 2.7510148704509296, Train Accuracy: 28.81%\n",
            "Accuracy on test set: 30.29%\n",
            "Epoch 8, Loss: 2.5054836757957477, Train Accuracy: 34.15%\n",
            "Accuracy on test set: 34.02%\n",
            "Epoch 9, Loss: 2.274147378209302, Train Accuracy: 39.26%\n",
            "Accuracy on test set: 34.53%\n",
            "Epoch 10, Loss: 2.07378155953439, Train Accuracy: 43.91%\n",
            "Accuracy on test set: 39.30%\n",
            "Epoch 11, Loss: 1.8885610077692114, Train Accuracy: 48.36%\n",
            "Accuracy on test set: 40.77%\n",
            "Epoch 12, Loss: 1.7504858158128647, Train Accuracy: 51.73%\n",
            "Accuracy on test set: 41.69%\n",
            "Epoch 13, Loss: 1.6092732509078882, Train Accuracy: 55.13%\n",
            "Accuracy on test set: 42.69%\n",
            "Epoch 14, Loss: 1.4906080412437848, Train Accuracy: 58.15%\n",
            "Accuracy on test set: 43.29%\n",
            "Epoch 15, Loss: 1.4007077008257132, Train Accuracy: 60.50%\n",
            "Accuracy on test set: 43.05%\n",
            "Epoch 16, Loss: 1.3238256530231223, Train Accuracy: 62.69%\n",
            "Accuracy on test set: 42.62%\n",
            "Epoch 17, Loss: 1.2433309305049574, Train Accuracy: 64.82%\n",
            "Accuracy on test set: 42.81%\n",
            "Epoch 18, Loss: 1.1924497780135221, Train Accuracy: 66.42%\n",
            "Accuracy on test set: 43.10%\n",
            "Epoch 19, Loss: 1.1473293638290347, Train Accuracy: 67.89%\n",
            "Accuracy on test set: 43.31%\n",
            "Epoch 20, Loss: 1.0954157691020185, Train Accuracy: 69.37%\n",
            "Accuracy on test set: 42.91%\n",
            "Epoch 21, Loss: 1.083153302254884, Train Accuracy: 69.64%\n",
            "Accuracy on test set: 44.32%\n",
            "Epoch 22, Loss: 1.0444199638750853, Train Accuracy: 71.17%\n",
            "Accuracy on test set: 43.54%\n",
            "Epoch 23, Loss: 1.0448409274715902, Train Accuracy: 71.25%\n",
            "Accuracy on test set: 44.51%\n",
            "Epoch 24, Loss: 1.0017287262794001, Train Accuracy: 72.65%\n",
            "Accuracy on test set: 42.55%\n",
            "Epoch 25, Loss: 0.9967814887423649, Train Accuracy: 72.98%\n",
            "Accuracy on test set: 41.84%\n",
            "Epoch 26, Loss: 0.9948117833994233, Train Accuracy: 73.28%\n",
            "Accuracy on test set: 43.22%\n",
            "Epoch 27, Loss: 1.0258426518010362, Train Accuracy: 72.68%\n",
            "Accuracy on test set: 42.59%\n",
            "Epoch 28, Loss: 1.0454130759629447, Train Accuracy: 72.56%\n",
            "Accuracy on test set: 42.29%\n",
            "Epoch 29, Loss: 1.1042328048545076, Train Accuracy: 71.24%\n",
            "Accuracy on test set: 42.98%\n",
            "Epoch 30, Loss: 1.098296227776791, Train Accuracy: 71.49%\n",
            "Accuracy on test set: 42.47%\n",
            "Epoch 31, Loss: 1.0787982832821434, Train Accuracy: 72.20%\n",
            "Accuracy on test set: 41.53%\n",
            "Epoch 32, Loss: 1.1394226798392317, Train Accuracy: 70.92%\n",
            "Accuracy on test set: 41.65%\n",
            "Epoch 33, Loss: 1.2017422568462695, Train Accuracy: 69.54%\n",
            "Accuracy on test set: 41.89%\n",
            "Epoch 34, Loss: 1.2728810056548594, Train Accuracy: 68.28%\n",
            "Accuracy on test set: 41.49%\n",
            "Epoch 35, Loss: 1.3665440497191057, Train Accuracy: 65.95%\n",
            "Accuracy on test set: 39.00%\n",
            "Epoch 36, Loss: 1.4744951919369076, Train Accuracy: 64.06%\n",
            "Accuracy on test set: 40.39%\n",
            "Epoch 37, Loss: 1.5048037314658884, Train Accuracy: 63.02%\n",
            "Accuracy on test set: 38.74%\n",
            "Epoch 38, Loss: 1.6588502586497675, Train Accuracy: 59.84%\n",
            "Accuracy on test set: 40.17%\n",
            "Epoch 39, Loss: 1.7572447354988674, Train Accuracy: 57.63%\n",
            "Accuracy on test set: 39.84%\n",
            "Epoch 40, Loss: 1.8279781963514246, Train Accuracy: 56.25%\n",
            "Accuracy on test set: 35.66%\n",
            "Epoch 41, Loss: 2.0209794843288336, Train Accuracy: 52.33%\n",
            "Accuracy on test set: 36.69%\n",
            "Epoch 42, Loss: 2.086994680144903, Train Accuracy: 50.33%\n",
            "Accuracy on test set: 36.67%\n",
            "Epoch 43, Loss: 2.2582717292449055, Train Accuracy: 46.83%\n",
            "Accuracy on test set: 35.20%\n",
            "Epoch 44, Loss: 2.402078789365871, Train Accuracy: 43.40%\n",
            "Accuracy on test set: 31.37%\n",
            "Epoch 45, Loss: 2.538984709840906, Train Accuracy: 40.63%\n",
            "Accuracy on test set: 31.26%\n",
            "Epoch 46, Loss: 2.687710897849344, Train Accuracy: 37.38%\n",
            "Accuracy on test set: 30.66%\n",
            "Epoch 47, Loss: 2.8017273043732507, Train Accuracy: 35.29%\n",
            "Accuracy on test set: 28.59%\n",
            "Epoch 48, Loss: 2.9604718996130903, Train Accuracy: 31.17%\n",
            "Accuracy on test set: 27.01%\n",
            "Epoch 49, Loss: 3.157839907404712, Train Accuracy: 27.19%\n",
            "Accuracy on test set: 23.36%\n",
            "Epoch 50, Loss: 3.3290310878582927, Train Accuracy: 23.35%\n",
            "Accuracy on test set: 21.02%\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    net.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    write_log(log_file_path, f\"VGG-16\\n{'-'*50}\\nEpoch {epoch + 1}:\\n\\tTrain loss: {train_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(trainloader)}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Evaluate the model in testing dataset and calculate the accuracy\n",
        "    net.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    write_log(log_file_path, f\"VGG-16\\n{'-'*50}\\nEpoch {epoch + 1}:\\n\\tAccuracy on test set: {test_accuracy:.2f}%\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - epoch_start_time\n",
        "    elapsed_time_str = f\"{int(elapsed_time // 3600)}h {int((elapsed_time % 3600) // 60)}m {int(elapsed_time % 60)}s\"\n",
        "    epoch_times_list.append((epoch + 1, elapsed_time))\n",
        "\n",
        "    # Calculate total elapsed time\n",
        "    total_elapsed_time = sum([epoch_times[1] for epoch_times in epoch_times_list])\n",
        "    total_elapsed_time_str = f\"{int(total_elapsed_time // 3600)}h {int((total_elapsed_time % 3600) // 60)}m {int(total_elapsed_time % 60)}s\"\n",
        "    write_log(log_file_path, f\"\\tElapsed Time: {elapsed_time_str}\")\n",
        "    write_log(log_file_path, f\"\\tTotal Elapsed Time: {total_elapsed_time_str}\\n{'-'*50}\")\n",
        "    print(f\"Accuracy on test set: {test_accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
